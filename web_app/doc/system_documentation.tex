\documentclass[10pt,german,a4]{report}
\usepackage[ngerman]{babel}
\usepackage[latin9]{inputenc}
\usepackage{graphicx}
\usepackage{float}
\usepackage{geometry}
\usepackage{extsizes}
\usepackage{listings}
\lstloadlanguages{sh}
\lstset{showstringspaces=false,
	breaklines = true,
	frame=single}

\geometry{a4paper,left=20mm,right=15mm,top=3cm,bottom=5cm}

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}
\renewcommand{\thechapter}{\Roman{chapter}}
\renewcommand{\thesection}{\arabic{section}}
\begin{document}
\title{Dokumentation des\\Wettkampfsystems der Software-Challenge}
\maketitle
\tableofcontents
\pagebreak

\chapter{Einführung}
Die Software-Challenge ist ein Schulprojekt, das durch das Institut für Informatik der CAU Kiel veranstaltet wird und das durch zahlreiche Unternehmen, die Prof. Dr. Werner Petersen-Stiftung sowie das Ministerium für Wissenschaft, Wirtschaft und Verkehr des Landes Schleswig-Holstein finanziert wird. In Zusammenarbeit mit den Gymnasien und Gesamtschulen und dem Institut für Informatik sowie den Firmen soll in diesem Projekt der Informatik-Unterricht in praxisbezogener Weise mitgestaltet und dadurch aufgewertet werden. Gegenstand der Software-Challenge ist ein Programmierwettbewerb, der während des gesamten Schuljahres läuft, und der den Schülerinnen und Schülern die Möglichkeit bietet, mit Spaß und Spannung sowie mit kompetenter Begleitung in die Welt der Informatik einzusteigen. Für die Verwaltung der Teams, die von den Schülern entwickelten Computerspieler (Clients) und die Abwicklung der Spieltage wird ein Online-Wettkampfsystem benutzt, das einen termingerechten und regelkonformen Ablauf der Spiele sicherstellt.

In dieser Dokumentation soll von technischer Seite das Zusammenspiel der Anwendungen und Prozesse des Wettkampfsystems erläutert werden. Dazu gehören der Webserver, der Spielserver und die virtuellen Maschinen zur Clientausführung. Weiterhin soll hier noch auf die Datensicherung eingegangen werden.
Für Informationen zur Software-Challenge im Allgemeinen, dem Wettkampfverlauf und der Anwendersoftware steht unter http://sc-doku.gfxpro.eu/wiki/Hauptseite ein Wiki zur Verfügung.
\chapter{Numerobis}\label{numerobis}
Auf Numerobis laufen die virtuellen Maschinen (VMs), auf denen die meisten Anwendungen der Software-Challenge ausgeführt werden. Zusätzlich gibt es einige Programme und Scripte, die helfen, die VMs zu steuern.
\section{Hardware und OS}
Numerobis besitzt 16 Xeon-Kerne mit jeweils 2,27GHz. Weiterhin verfügt er über 24GB RAM.
Als Betriebssystem wird SunOS genutzt, genauer:
\begin{lstlisting}[language=sh]
  > uname -a
  SunOS numerobis 5.10 Generic_142901-04 i86pc i386 i86pc
\end{lstlisting}
\section{Netzplan}\label{netzplan}
\begin{figure}[H]
  \includegraphics[width=0.8\textwidth]{netzplan.eps}
  \caption{Netzplan von Numerobis}
  \label{fig:netzplan}
\end{figure}
\section{Software}
\subsection{VirtualBox}
VirtualBox wird genutzt um virtuelle Maschinen bereitzustellen, auf denen das Websystem und die Computerspieler ausgeführt werden.
\subsubsection{Steuern der VMs}\label{vmcontrol} 
Hier ein paar grundlegende Befehle und Scripte zur VM-Steuerung. Für alles Weitere sind die Hilfe des Befehls VBoxManage und Dokumentationen zu VirtualBox zu Rate zu ziehen.
\subsubsection*{Informationsgewinnung}
\begin{lstlisting}[language=sh]
  VBoxManage list runningvms
  VBoxManage list vms
  VBoxManage list hdds
\end{lstlisting}
Listet alle
\begin{itemize}
  \item laufenden VMs
  \item registrierten VMs
  \item registrierten Festplatten
\end{itemize}
auf.
\begin{lstlisting}[language=sh]
  VBoxManage showvminfo <VM>
  VBoxManage guestproperty enumerate
\end{lstlisting}
Zeigt an:
\begin{itemize}
  \item Hardwareinformationen der angegebenen VM
  \item Informationen des (laufenden) Gastsystems
\end{itemize}
\subsubsection*{Modifizierung}
\begin{lstlisting}[language=sh]
  VBoxManage createvm -name <VM> -register --ostype <OS>
  VBoxManage unregistervm <VM> [--delete]
\end{lstlisting}
\begin{itemize}
  \item Erstellt und registriert eine neue VM mit dem Namen VM und dem angegebenen Betriebssystem (z.B. Ubuntu)
  \item Hebt die Registrierung der VM auf [und löscht diese]
\end{itemize}
\begin{lstlisting}[language=sh]
  VBoxManage modifyvm <VM> --memory <MEM> --cpus <CPUS>
  VBoxManage modifyvm <VM> --nic1 hostonly --nictype1 82540EM --cableconnected1 on --hostonlyadapter1 <IF> --macaddress1 auto
  VBoxManage modifyvm <VM> --hda ``/path/to/image.vdi"
  VBoxManage modifyvm $vmname --hda none
\end{lstlisting}
Ändert die Hardware der angegebenen VM:
\begin{itemize}
  \item Setzt den RAM auf MEM MB und die Anzahl der CPUs auf CPUS.
  \item Richtet einen Netzwerkadapter für das Interface IF ein. Neben hostonly ist zum Beispiel auch bridged möglich, um der VM die Benutzung eines Host-Netzwerkinterfaces zu ermöglichen.
  \item Setzt die Festplatte hda der VM auf das angegebene Image.
  \item Entfernt die Festplatte hda aus der VM.
\end{itemize}
\begin{lstlisting}[language=sh]
  VBoxManage internalcommands sethduuid ``/path/to/image.vdi''
  VBoxManage closemedium disk ``/path/to/image.vdi''
\end{lstlisting}
\begin{itemize}
  \item Ändert die UUID des angegebenen Disk-Images
  \item Hebt die Registrierung des angegebenen Festplattenimages auf
\end{itemize}
\subsubsection*{Steuerung}
\begin{lstlisting}[language=sh]
  VBoxManage startvm <VM> --type headless
  VBoxManage startvm <VM> --type vrdp
\end{lstlisting}
Startet die VM mit dem Namen $<$VM$>$ 
\begin{itemize}
  \item ohne grafische Oberfläche 
  \item mit VRDP Schnittstelle zur Benutzung eines Remote Desktops.
\end{itemize}
\begin{lstlisting}[language=sh]
  VBoxManage controlvm <VM> poweroff
\end{lstlisting}
Stoppt die VM mit dem Namen $<$VM$>$
\begin{lstlisting}[language=sh] 
  ~/kill_vms.sh
\end{lstlisting}
Beendet alle laufenden Client VMs und die dazugehörigen Startscripte. Dieses Script sollte mit Vorsicht benutzt werden, da es die VM-Prozesse rücksichtslos beendet ohne die VM oder deren HDD zu deregistrieren und zu löschen.
\subsubsection{Die VMMain starten}
Die VMMain muss zum Beispiel nach einem Neustart von Numerobis gestartet werden. Dies geschieht mit dem Befehl
\begin{lstlisting}[language=sh]
  VBoxManage startvm vmmain --type headless
\end{lstlisting}
\subsubsection{Eine geklonte Client-VM starten}
Da jeder Client auf einer eigenen Version der Client-VM gestartet werden soll, muss für jeden Client zunächst ein Klon der Client-VM erstellt und gestartet werden. Auf Numerobis übernimmt dies das Script /home/vbox/startVM.sh (siehe Anhang \ref{anhang:startVM.sh}), trotzdem sollen hier im Einzelnen die nötigen Befehle erläutert werden.
Zunächst müssen einige Dinge vorbereitet werden. Dazu gehören ein eindeutiger Name (z.B. der aktuelle Timestamp) des neuen Klons und der (ZFS-)Pfad zum Klon:
\begin{lstlisting}[language=sh]
  # Unique number for the VM
  vmnr=`bin/date +%m%d%H%M%S`
  # Path for VM harddisk
  zfspath=``zpool1/vbox/harddisks/vmclient-$vmnr''
  vmpath=``/home/vbox/harddisks/vmclient-$vmnr''
\end{lstlisting}
Nun muss die Image-Vorlage der Client-VM vom Snapshot geklont werden. Dies geschieht folgendermaßen:
\begin{lstlisting}[language=sh]
  zfs clone zpool1/vbox/harddisks/vmclient@kopie $zfspath
\end{lstlisting}
Die geklonte HDD braucht unbedingt eine eigene UUID, damit VirtualBox alles auseinanderhalten kann:
\begin{lstlisting}[language=sh]
  cd $vmpath
  VBoxManage internalcommands sethduuid vmclient.vdi
\end{lstlisting}
Nun kann eine neue VM erstellt werden. Dann muss die Hardware, Netzwerkverbindung und Festplatte hinzugefügt werden:
\begin{lstlisting}[language=sh]
  VBoxManage createvm -name $vmname -register --ostype Ubuntu
  VBoxManage modifyvm $vmname --memory 1536 --cpus 1
  VBoxManage modifyvm $vmname --nic1 hostonly --nictype1 82540EM --cableconnected1 on --hostonlyadapter1 vboxnet0 --macaddress1 auto
  VBoxManage modifyvm $vmname --hda ``$vmpath/vmclient.vdi"
\end{lstlisting}
Die erstellte VM kann nun gestartet und benutzt werden:
\begin{lstlisting}[language=sh]
  VBoxManage startvm $vmname --type headless
\end{lstlisting}
\subsubsection*{Den Klon wieder vernichten}
Wird der Klon nicht mehr benötigt, sollte dieser wieder gelöscht werden. Dazu muss man die geklonte Client-VM zunächst wieder beenden. Dann wird die Festplatte entfernt und die Maschine gelöscht. Schließlich muss die HDD noch ausgeworfen werden.
\begin{lstlisting}[language=sh]
  VBoxManage controlvm $vmname poweroff
  VBoxManage modifyvm $vmname --hda none
  VBoxManage unregistervm $vmname --delete
  VBoxManage closemedium disk ``$vmpath/vmclient.vdi''
\end{lstlisting}
Zuletzt noch das Festplatten-Image aus dem Dateisystem löschen:
\begin{lstlisting}[language=sh]
  zfs destroy -f $zfspath
\end{lstlisting}
\subsection{Der Consumer}
Der Consumer läuft ständig im Hintergrund. In regelmäßigen Abständen prüft er, ob eine neue Anfrage zum Starten einer Client-VM vorliegt. Dazu pollt er eine Queue, die mit ``vm-queue'' bezeichnet ist (mehr zu den Queues bei \ref{queues}). Liegt eine solche Anfrage vor, startet der Consumer einen neuen Klon der Client-VM. Dazu benutzt er das Startscript /startVM.sh (siehe \ref{anhang-startVM.sh}).
\subsubsection*{Consumer starten}
Der Consumer kann über das Script /home/vbox/startVMConsumer.sh gestartet werden. Dies muss nach einem Neustart von Numerobis manuell durchgeführt werden. Die Startparameter für den Consumer (/home/vbox/daemonconsumer.jar) sind:
\begin{itemize}
  \item -c: Der Befehl oder das Script der/das ausgeführt werden soll.
  \item -b: Pfad zur Bash
  \item -q: Name der Queue, die überprüft werden soll
  \item -h: Host, auf dem die Queue läuft
  \item -m: Maximale Anzahl an Aufträgen, die gleichzeitig bearbeitet werden dürfen
  \item -i: Intervall in Sekunden, in dem die Queue überprüft werden soll
  \item -d: Debug-Modus, erzeugt erweiterte Ausgaben
\end{itemize}
\subsection{Das ZFS Dateisystem}\label{filesystem}
Zur Verwaltung der Disk-Images für die VMs wird das ZFS Dateisystem und dessen Features zum Klonen genutzt. Das Ausgangsdateisystem für alle Disk-Images ist /home/vbox/harddisks.
Ab hier muss für jedes Diskimage ein eigenes weiteres Filesystem angelegt werden. Diese Images sollen NIE direkt benutzt werden. Stattdessen sollte immer erst ein Snapshot/Clone angelegt werden. Das hat den Vorteil, dass das Original immer als Fallback zur Verfügung steht.
\subsubsection*{Beispiel für OpenSolaris} 
Zuerst muss ein neues Filesystem angelegt werden.
\begin{lstlisting}[language=sh]
% zfs create zpool1/vbox/harddisks/opensolaris
% zfs list -r zpool1
NAME                                      USED  AVAIL  REFER  MOUNTPOINT
zpool1                                   13.2G   121G  6.97G  /zpool1
zpool1/vbox                              6.24G   121G  4.19G  /zpool1/vbox
zpool1/vbox/harddisks                    2.05G   121G    23K  /zpool1/vbox/harddisks
zpool1/vbox/harddisks/opensolaris        2.05G   121G  2.05G  /zpool1/vbox/harddisks/opensolaris
\end{lstlisting}
Dann erst kann das Image dort abgelegt werden.
\begin{lstlisting}[language=sh]
% cd ~/harddisks/opensolaris
% cp <irgendwoher>/OpenSolaris.vdi .
\end{lstlisting}
ACHTUNG: Jedes Image braucht eine eigene UUID! Sonst bringt die VirtualBox u.U. die Images durcheinander.
\begin{lstlisting}[language=sh]
% VBoxManage internalcommands sethduuid OpenSolaris.vdi
VirtualBox Command Line Management Interface Version 3.0.8
(C) 2005-2009 Sun Microsystems, Inc.
All rights reserved.

UUID changed to: cd6f93b1-cefd-405a-88ba-b55661928bc3
\end{lstlisting}
Jetzt wird zuerst ein Snapshot des Originals angelegt. Das muss sein, da Klone nur von Snapshots erzeugt werden können.
\begin{lstlisting}[language=sh]
% zfs snapshot zpool1/vbox/harddisks/opensolaris@kopie
% zfs list -r zpool1
NAME                                      USED  AVAIL  REFER  MOUNTPOINT
zpool1                                   13.2G   121G  6.97G  /zpool1
zpool1/vbox                              6.24G   121G  4.19G  /zpool1/vbox
zpool1/vbox/harddisks                    2.05G   121G    23K  /zpool1/vbox/harddisks
zpool1/vbox/harddisks/opensolaris        2.05G   121G  2.05G  /zpool1/vbox/harddisks/opensolaris
zpool1/vbox/harddisks/opensolaris@kopie      0      -  2.05G  -
\end{lstlisting}
Und dann den Clone erzeugen:
\begin{lstlisting}[language=sh]
% zfs clone zpool1/vbox/harddisks/opensolaris@kopie zpool1/vbox/harddisks/opensolaris-1
% zfs list -r zpool1
NAME                                      USED  AVAIL  REFER  MOUNTPOINT
zpool1                                   13.2G   121G  6.97G  /zpool1
zpool1/vbox                              6.24G   121G  4.19G  /zpool1/vbox
zpool1/vbox/harddisks                    2.05G   121G    23K  /zpool1/vbox/harddisks
zpool1/vbox/harddisks/opensolaris        2.05G   121G  2.05G  /zpool1/vbox/harddisks/opensolaris
zpool1/vbox/harddisks/opensolaris@kopie      0      -  2.05G  -
zpool1/vbox/harddisks/opensolaris-1          0   121G  2.05G  /zpool1/vbox/harddisks/opensolaris-1
\end{lstlisting}
Dieser Clone kann beschrieben werden, und sollte sofort mit einer eigenen UUID versorgt werden.
\begin{lstlisting}[language=sh]
%  cd ../*1
/home/vbox/harddisks/opensolaris-1

% ls -l
-rw-------   1 vbox     vbox     3845210624 Oct 15 12:42 OpenSolaris.vdi

% VBoxManage internalcommands sethduuid OpenSolaris.vdi
VirtualBox Command Line Management Interface Version 3.0.8
(C) 2005-2009 Sun Microsystems, Inc.
All rights reserved.

UUID changed to: fcafca0b-3e77-4259-a954-01d4ab5e4954

% zfs list -r zpool1
NAME                                      USED  AVAIL  REFER  MOUNTPOINT
zpool1                                   13.2G   121G  6.97G  /zpool1
zpool1/vbox                              6.24G   121G  4.19G  /zpool1/vbox
zpool1/vbox/harddisks                    2.05G   121G    24K  /zpool1/vbox/harddisks
zpool1/vbox/harddisks/opensolaris        2.05G   121G  2.05G  /zpool1/vbox/harddisks/opensolaris
zpool1/vbox/harddisks/opensolaris@kopie      0      -  2.05G  -
zpool1/vbox/harddisks/opensolaris-1      54.5K   121G  2.05G  /zpool1/vbox/harddisks/opensolaris-1
\end{lstlisting}
Wie man gut erkennen kann, belegt der Clone statt 2GB nur 55KB und kann trotzdem völlig unabhängig vom Original genutzt werden.
Bei Bedarf ist schnell eine zweite Kopie erstellt:
\begin{lstlisting}[language=sh]
% zfs clone zpool1/vbox/harddisks/opensolaris@kopie zpool1/vbox/harddisks/opensolaris-2
% cd ../*2
% VBoxManage internalcommands sethduuid OpenSolaris.vdi
VirtualBox Command Line Management Interface Version 3.0.8
(C) 2005-2009 Sun Microsystems, Inc.
All rights reserved.

UUID changed to: 55cec833-3685-483b-9a29-e90cf9b33800

% zfs list -r zpool1
NAME                                      USED  AVAIL  REFER  MOUNTPOINT
zpool1                                   13.2G   121G  6.97G  /zpool1
zpool1/vbox                              6.24G   121G  4.19G  /zpool1/vbox
zpool1/vbox/harddisks                    2.05G   121G    25K  /zpool1/vbox/harddisks
zpool1/vbox/harddisks/opensolaris        2.05G   121G  2.05G  /zpool1/vbox/harddisks/opensolaris
zpool1/vbox/harddisks/opensolaris@kopie      0      -  2.05G  -
zpool1/vbox/harddisks/opensolaris-1      54.5K   121G  2.05G  /zpool1/vbox/harddisks/opensolaris-1
zpool1/vbox/harddisks/opensolaris-2      54.5K   121G  2.05G  /zpool1/vbox/harddisks/opensolaris-2
\end{lstlisting}
\chapter{Die Client-VM}
Die Client-VMs dienen dazu, die Computerspieler in einer isolierten und genau festgelegten Umgebung auszuführen. Für jeden auszuführenden Computerspieler wird ein neuer Klon der Client-VM erstellt und gestartet.
\section{Konzept}
Nach dem Boot der Client-VM wird auf ihr automatisch ein weiterer Consumer (/home/scadmin/swcconsumer.jar) gestartet, der einen Eintrag aus der Queue ``swc-job-queue'' aus\-liest (mehr zu den Queues bei \ref{queues}). Dieser Eintrag enthält einen SCP-Befehl, der den vorbereiteten Client von der VMMain auf die Client-VM kopiert. Dort wird der Client entpackt und ausgeführt. 
\section{Hardware}
Jede Client-VM läuft mit einer Xeon 2,27GHz (1 Kern) CPU und 1,5 GB RAM. Als Betriebssystem wird ein 32-bit Ubuntu benutzt, genauer:
\begin{lstlisting}[language=sh]
  > uname -a
  Linux base 2.6.31-14-generic #48-Ubuntu SMP Fri Oct 16 14:04:26 UTC 2009 i686 GNU/Linux
\end{lstlisting}
\section{Software}
Ein Eintrag in /etc/rc.local startet nach dem Booten den Consumer:
\begin{lstlisting}[language=sh]
  /usr/bin/java -jar /home/scadmin/swcconsumer.jar 192.168.56.2
\end{lstlisting}
Dieser liest aus der ``swc-job-queue'' den nächsten Eintrag (einen SCP-Befehl) aus und übergibt diesen an das Consumer-Script /home/scadmin/consume.sh. Dieses bereitet zunächst den Ordner vor, in dem der Client später ausgeführt werden soll:
\begin{lstlisting}[language=sh]
  /bin/mkdir $clientdir
  /bin/chown clientexec:clientexec $clientdir
  /bin/chmod 777 $clientdir
\end{lstlisting}
Dann führt es den übergebenen SCP-Befehl aus und kopiert damit das Client-Archiv von der VMMain in den richtigen Ordner auf der Client-VM:
\begin{lstlisting}[language=sh]
  `$scpcommand $zipfile` >> $log 2>&1
  Führt zu:
  /usr/bin/scp -i /home/scadmin/id_rsa scadmin@192.168.56.2:/home/scadmin/tmp/1286904225_400_gymnasium-elmschenhagen_634.zip /home/clientexec/client/client.zip
\end{lstlisting}
Danach löscht das Script das Client-Archiv auf der VMMain:
\begin{lstlisting}[language=sh]
  sudo -u scadmin ssh -l scadmin 192.168.56.2 rm /home/scadmin/tmp/${file} >> $log 2>&1 --
\end{lstlisting}
Jetzt wird das Client-Archiv entpackt und dem Benutzer ``clientexec'' gegeben, unter dem der Client ausgeführt werden soll. Außerdem muss noch das Startscript des Clients (startup.sh) ausführbar gemacht werden:
\begin{lstlisting}[language=sh]
  cd $clientdir
  /usr/bin/unzip $zipfile >> $log 2>&1
  /bin/chown -R clientexec:clientexec .
  /bin/chmod +x $startup
\end{lstlisting}
Schließlich wird der Client mit dem Benutzer ``clientexec'' ausgeführt. Der Benutzerwechsel stellt sicher, dass der Client keinen Zugang zu sensiblen Daten oder gar SSH-Zugang zur VMMain erhält.
\begin{lstlisting}[language=sh]
  sudo -Hu clientexec /bin/bash +x $startup >> $log 2>&1 --
\end{lstlisting}
\section{Die Client-VM verändern}
Möchte man die Vorlage der Client-VM verändern, muss man die Client-VM selbst starten (auf Numerobis):
\begin{lstlisting}[language=sh]
  VBoxManage startvm vmclient --type headless
\end{lstlisting}
Nun braucht die VM einige Zeit zum Hochfahren. Ist die VM hochgefahren, kann man die IP der VM auslesen:
\begin{lstlisting}[language=sh]
  VBoxManage guestproperty enumerate vmclient
\end{lstlisting}
Entscheidend ist folgende Zeile:
\begin{lstlisting}[language=sh]
  Name: /VirtualBox/GuestInfo/Net/0/V4/IP, value: 192.168.56.56, timestamp: 1286982333177594000, flags:
\end{lstlisting}
Ist diese Zeile noch nicht in der Ausgabe vorhanden, ist die VM noch nicht vollständig hochgefahren.
Nun kann man sich (am besten von der VMMain aus) per SSH auf die VMClient verbinden und alle nötigen Änderungen vornehmen.
Danach sollte man die VMClient wieder herunterfahren. Vorzugsweise nicht einfach abschalten, sondern richtig Herunterfahren:
\begin{lstlisting}[language=sh]
  sudo shutdown -P now
\end{lstlisting}
Nun muss noch die Klonvorlage im ZFS-Dateisystem aktualisiert werden. Dazu muss zunächst der bereits vorhandene Snapshot gelöscht werden:
\begin{lstlisting}[language=sh]
  zfs destroy -R zpool1/vbox/harddisks/vmclient@kopie
\end{lstlisting}
Dann wird das HDD-Image der ClientVM in das ZFS-Dateisystem kopiert:
\begin{lstlisting}[language=sh]
  cd /home/vbox/harddisks/vmclient
  cp /home/vbox/harddisks/vm/vmclient.vdi .
\end{lstlisting}
Und zuletzt ein neuer Snapshot erstellt:
\begin{lstlisting}[language=sh]
  zfs snapshot zpool1/vbox/harddisks/vmclient@kopie
\end{lstlisting}
Das Aktualisieren des Snapshots wird komplett auch vom Script ``/home/vbox/updateVMClient.sh'' (siehe Anhang \ref{anhang:updateVMClient.sh}) übernommen.
\chapter{Die VMMain}
\section{Konzept}
Auf der VMMain läuft die Rails-Webanwendung des Wettkampfsystems sowie der Software-Challenge Server, auf dem die Spiele ausgetragen werden.
\section{Hardware}
Die VMMain verfügt über eine Xeon CPU mit 2,27GHz und ~3,5GB RAM. Als Betriebssystem wird Ubuntu verwendet, genauer:
\begin{lstlisting}[language=sh]
  > uname -a
  Linux base 2.6.32-25-generic #45-Ubuntu SMP Sat Oct 16 19:48:22 UTC 2010 i686 GNU/Linux
\end{lstlisting}
\section{Software}
\subsection{Die WebApp}
Die Webanwendung (WebApp) ist eine Rails-Anwendung, die über den Apache Webserver bei Bedarf automatisch gestartet wird. Die WebApp wird allerdings durch einige Anwendungen ergänzt, die gesondert gestartet werden müssen.
\subsubsection{Der Job-Worker}
Zur geplanten Abarbeitung von Aufgaben im Hintergrund (zum Beispiel das Durchführen von Spielen) wird der DelayedJob-Worker von Rails genutzt. Damit dieser seine Aufgaben auch erfüllt, müssen die Worker-Prozesse laufen. Nach einem Neustart der VMMain müssen diese evtl. manuell gestartet werden. Dies geschieht folgendermaßen:
\begin{lstlisting}[language=sh]
  cd /home/scadmin/rails-workspace
  cap delayed_job:start
\end{lstlisting}
\subsubsection{Daemons}
Es gibt einige Daemons, die ständig im Hintergrund parallel zur WebApp laufen, z.B. um den Server zu Überwachen und automatische Abläufe in der WebApp zu steuern. Diese Daemons müssen evtl. nach einem Neustart der VMMain manuell gestartet werden. Dies geschieht folgendermaßen:
\begin{lstlisting}[language=sh]
  cd /home/scadmin/rails-workspace
  cap daemons:start
\end{lstlisting}
\subsection{Der Spielserver}
Der Spielserver ist eine Java-Anwendung, die ständig läuft. Sie nimmt Verbindungen von der WebApp entgegen, um neue Spiele zu öffnen, auf die sich dann die auf den Client-VMs ausgeführten Computerspieler verbinden können.
Der Spielserver wird automatisch zusammen mit den Daemons gestartet. Die Anwendung befindet sich in /home/scadmin/rails-deployment/current/public/server/.
\subsection{Die Queues}\label{queues}
Eine Warteschlange (RabbitMQ) wird genutzt, um Aufträge für zu startende VMs zu verwalten. Auf der VMMain werden Einträge in die Warteschlange eingefügt, die auf Numerobis und den Client-VMs ausgelesen werden. Ein Eintrag in die Warteschlange ``vm-queue'' hat zur Folge, dass eine neue Client-VM auf Numerobis gestartet wird. Der Inhalt des Eintrags ist dabei egal.
Gleichzeitig sollte in die Warteschlange ``swc-job-queue'' ein Eintrag eingetragen werden, der einen SCP-Befehl zum Kopieren des vorbereiteten Clients auf die Client-VM enthält. Dieser sieht z.B. so aus:
\begin{lstlisting}[language=sh]
  /usr/bin/scp -i /home/scadmin/id_rsa scadmin@192.168.56.2:/home/scadmin/tmp/1286904225_400_gymnasium-elmschenhagen_634.zip
\end{lstlisting}
Das Eintragen der entsprechenden Elemente in die Queues wird auf der VMMain vom Producer (s. \ref{producer}) übernommen.
\subsection{Der Producer}\label{producer} 
Der Producer überwacht ein angegebenes Verzeichnis. Findet er in diesem einen vorbereiteten Client (ZIP-Archiv), verschiebt er diesen in einen spezifizierten Ordner und erstellt jeweils einen Eintrag in die Queues ``vm-queue'' und ``swc-job-queue''. Derzeit ist der überwachte Ordner /home/scadmin/clients/ und der Ordner, in dem die Ordner auf die VM warten ist /home/scadmin/tmp.
\subsubsection*{Producer starten}  
Der Producer kann über das Script ~/startProducer.sh gestartet werden. Dieses Script ruft god auf, ein Pro\-zess\-ü\-ber\-wach\-ungs\-tool, das sicherstellt, das der Producer ständig läuft und bei einem Absturz neugestartet wird. Der Aufruf sieht folgendermaßen aus (einmal mit god und einmal direkt)
\begin{lstlisting}[language=sh]
  god -c ~/monitoring/producer.conf # Aufruf mit god
  java -jar /home/scadmin/producer.jar <Startparameter> # Direkter Aufruf, unüberwacht
\end{lstlisting}
Der Producer muss zum Beispiel nach einem Neustart der VMMain evtl. manuell gestartet werden. Die Startparameter für den Producer (producer.jar) sind:
\begin{itemize}
  \item -w: Verzeichnis, das überwacht werden soll
  \item -s: Der SCP-Befehl zum Kopieren des Clients auf die Client-VM
  \item -h: Host auf dem die RabbitMQ läuft
  \item -t: Das Verzeichnis, in das der behandelte Client verschoben werden soll.
\end{itemize}
\chapter{Anhang}
\section{startVM.sh}\label{anhang:startVM.sh}
\begin{lstlisting}[language=sh]
#!/bin/bash

message=$1
log="/home/vbox/logs/script.log"
/bin/echo "Starting a new VM at \frac{bin/date`" >> $log

# Unique number for the VM
vmnr=\frac{bin/date +%m%d%H%M%S`
# Path for VM harddisk
zfspath="zpool1/vbox/harddisks/vmclient-$vmnr"
vmpath="/home/vbox/harddisks/vmclient-$vmnr"

# Clone the VM HDD template
echo "creas sethduuid vmclient.vdi
cd

# Create and start new VM using the cloned HDD
/bin/echo "creating vm"
vmname="vmclient-$vmnr"
VBoxManage createvm -name $vmname -register --ostype Ubuntu
VBoxManage modifyvm $vmname --memory 1536 --cpus 1
VBoxManage modifyvm $vmname --nic1 hostonly --nictype1 82540EM --cableconnected1 on --hostonlyadapter1 vboxnet0 --macaddress1 auto
VBoxManage modifyvm $vmname --hda "$vmpath/vmclient.vdi"

/bin/echo "starting vm"
VBoxManage startvm $vmname --type headless

# Give the VM 5 minutes to run before killing it again
/bin/echo "5"
sleep 60
/bin/echo "4"
sleep 60
/bin/echo "3"
sleep 60
/bin/echo "2"
sleep 60
/bin/echo "1"
sleep 60 
/bin/echo "0"

# Parse the VM's ip and call the log copy script on VMMain
echo "saving log file"
echo \varepsiloncho $vmname`
echo `VBoxManage guestproperty get $vmname /VirtualBox/GuestInfo/Net/0/V4/IP` 
vmip=`VBoxManage guestproperty get $vmname /VirtualBox/GuestInfo/Net/0/V4/IP | grep 'Value:' | sed 's poweroff
VBoxManage modifyvm $vmname --hda none
VBoxManage unregistervm $vmname --delete

# Destroy the HDD clone
zfs destroy -f $zfspath

exit 0
\end{lstlisting}
\section{updateVMClient.sh}\label{anhang:updateVMClient.sh}
\begin{lstlisting}[language=sh]
#!/bin/bash
# Update the hdd clone
echo ``removing snapshot''
zfs destroy -R zpool1/vbox/harddisks/vmclient@kopie

echo ``copying new .vdi''
cd ~/harddisks/vmclient
cp ~/harddisks/vm/vmclient.vdi .

echo ``creating snapshot''
zfs snapshot zpool1/vbox/harddisks/vmclient@kopie

exit 0
\end{lstlisting}
\end{document}
